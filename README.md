# üé¨ AI Video Dubber ‚Äî English to Hindi Dubbing Pipeline

A modular, production-ready Python pipeline that takes an English video and produces a Hindi-dubbed version with **voice cloning**, **lip synchronization**, and **face restoration** ‚Äî all using **free, open-source tools**.

> **Built for the Supernan AI Automation Intern Challenge**
> "The Golden 15 Seconds" ‚Äî 15 seconds of perfection.

---

## üèóÔ∏è Architecture

```
dub_video.py                 ‚Üê Main orchestrator with CLI
‚îÇ
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ video_utils.py       ‚Üê FFmpeg-based video/audio I/O
‚îÇ   ‚îú‚îÄ‚îÄ transcription.py     ‚Üê Whisper speech-to-text
‚îÇ   ‚îú‚îÄ‚îÄ translation.py       ‚Üê IndicTrans2 / Google Translate
‚îÇ   ‚îú‚îÄ‚îÄ tts.py               ‚Üê XTTS v2 voice cloning
‚îÇ   ‚îú‚îÄ‚îÄ alignment.py         ‚Üê Audio duration matching
‚îÇ   ‚îú‚îÄ‚îÄ lipsync.py           ‚Üê Wav2Lip lip synchronization
‚îÇ   ‚îî‚îÄ‚îÄ enhancement.py       ‚Üê GFPGAN face restoration
‚îÇ
‚îú‚îÄ‚îÄ setup.py                 ‚Üê Automated environment setup
‚îú‚îÄ‚îÄ requirements.txt         ‚Üê Python dependencies
‚îú‚îÄ‚îÄ AI_Video_Dubber.ipynb    ‚Üê Google Colab notebook (one-click)
‚îî‚îÄ‚îÄ outputs/                 ‚Üê Pipeline outputs (auto-created)
```

### Pipeline Flow

```
Input Video (full)
      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Extract 15s clip ‚îÇ  ‚Üê FFmpeg (frame-accurate re-encode)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Extract audio    ‚îÇ  ‚Üê FFmpeg ‚Üí 16kHz mono WAV
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Transcribe       ‚îÇ  ‚Üê OpenAI Whisper (word timestamps)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Translate to     ‚îÇ  ‚Üê IndicTrans2 (context-aware)
‚îÇ    Hindi            ‚îÇ     or Google Translate (fallback)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Voice cloning    ‚îÇ  ‚Üê Coqui XTTS v2 (speaker matching)
‚îÇ                     ‚îÇ     or Edge TTS (no-GPU fallback)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. Audio alignment  ‚îÇ  ‚Üê librosa time-stretch + silence trim
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 7. Lip sync         ‚îÇ  ‚Üê Wav2Lip (GAN model)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 8. Face enhancement ‚îÇ  ‚Üê GFPGAN v1.4 (face restoration)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
   Final Dubbed Video
```

---

## üöÄ Quick Start

### Option 1: Google Colab (Recommended ‚Äî Free GPU)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com)

1. Open `AI_Video_Dubber.ipynb` in Google Colab
2. Set runtime to **GPU** (Runtime ‚Üí Change runtime type ‚Üí T4 GPU)
3. Run all cells ‚Äî the notebook handles everything

### Option 2: Local / Cloud VM

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/AI-video-dubber.git
cd AI-video-dubber

# Run automated setup
python setup.py

# Verify installation
python setup.py --check

# Run the pipeline
python dub_video.py --input video.mp4 --start 15 --end 30
```

---

## üìã Setup Instructions

### Prerequisites

| Requirement | Minimum | Recommended |
|-------------|---------|-------------|
| Python | 3.10+ | 3.10 |
| GPU VRAM | 4 GB | 8 GB+ |
| RAM | 8 GB | 16 GB |
| FFmpeg | Any | 5.0+ |
| Disk Space | 10 GB | 20 GB |

### Step-by-Step Setup

#### 1. Install FFmpeg

```bash
# Ubuntu/Debian
sudo apt-get update && sudo apt-get install -y ffmpeg

# macOS
brew install ffmpeg

# Windows (via Chocolatey)
choco install ffmpeg

# Google Colab (pre-installed)
```

#### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

#### 3. Setup Wav2Lip (for lip-sync)

```bash
python setup.py --setup-wav2lip
```

Then manually download `wav2lip_gan.pth`:
- [Download Link](https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/EdjI7bZlgApMqsVoEUUXpLsBxqXbn5z8VTmoxp55YNDcIA?e=n9ljGW)
- Place in: `Wav2Lip/checkpoints/wav2lip_gan.pth`

#### 4. Verify Setup

```bash
python setup.py --check
```

---

## üéØ Usage

### Basic Usage

```bash
# Process the challenge segment (0:15 - 0:30)
python dub_video.py --input supernan_training.mp4 --start 15 --end 30
```

### Advanced Options

```bash
# Use IndicTrans2 for better translation quality
python dub_video.py --input video.mp4 --translation indictrans2

# Use Edge TTS (no GPU needed for TTS stage)
python dub_video.py --input video.mp4 --tts edge

# Adjust speech rate (Hindi is ~10% longer than English)
python dub_video.py --input video.mp4 --speech-rate 1.1

# Skip heavy stages for quick testing
python dub_video.py --input video.mp4 --skip-lipsync --skip-enhancement

# Use smaller Whisper model for faster transcription
python dub_video.py --input video.mp4 --whisper-model base

# Lower Wav2Lip batch size for less VRAM usage
python dub_video.py --input video.mp4 --wav2lip-batch-size 4
```

### Full CLI Reference

```
python dub_video.py --help

Arguments:
  --input, -i          Path to input video (required)
  --start, -s          Start time in seconds (default: 15)
  --end, -e            End time in seconds (default: 30)
  --output-dir, -o     Output directory (default: outputs/)
  --whisper-model      tiny|base|small|medium|large (default: small)
  --translation        indictrans2|google|seamless (default: google)
  --tts                xtts|edge (default: xtts)
  --speech-rate        Speed multiplier (default: 1.05)
  --enhancement        gfpgan|codeformer (default: gfpgan)
  --skip-enhancement   Skip face restoration
  --skip-lipsync       Skip lip synchronization
  --wav2lip-dir        Wav2Lip repo path (default: Wav2Lip/)
  --wav2lip-batch-size Frames per batch (default: 16)
```

---

## üìÇ Output Structure

After running the pipeline, `outputs/` will contain:

```
outputs/
‚îú‚îÄ‚îÄ 01_clip.mp4              ‚Üê Extracted 15-second clip
‚îú‚îÄ‚îÄ 02_audio.wav             ‚Üê Original English audio
‚îú‚îÄ‚îÄ 03_transcription.json    ‚Üê Whisper transcription + timestamps
‚îú‚îÄ‚îÄ 04_translation.json      ‚Üê Hindi translation + segments
‚îú‚îÄ‚îÄ 05_hindi_raw.wav         ‚Üê Generated Hindi speech
‚îú‚îÄ‚îÄ 06_hindi_aligned.wav     ‚Üê Duration-matched Hindi audio
‚îú‚îÄ‚îÄ 07_lipsynced.mp4         ‚Üê Wav2Lip output
‚îú‚îÄ‚îÄ 08_enhanced.mp4          ‚Üê GFPGAN enhanced output
‚îú‚îÄ‚îÄ final_dubbed.mp4         ‚Üê ‚úÖ Final result
‚îú‚îÄ‚îÄ pipeline_config.json     ‚Üê Run configuration
‚îî‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ pipeline_*.log       ‚Üê Detailed execution log
```

---

## üí∞ Cost Analysis

### Current Pipeline (Free Tier)

| Component | Tool | Cost |
|-----------|------|------|
| Transcription | Whisper (local) | **‚Çπ0** |
| Translation | Google Translate / IndicTrans2 | **‚Çπ0** |
| Voice Cloning | Coqui XTTS v2 | **‚Çπ0** |
| Lip Sync | Wav2Lip | **‚Çπ0** |
| Face Enhancement | GFPGAN | **‚Çπ0** |
| Compute | Google Colab Free | **‚Çπ0** |
| **Total** | | **‚Çπ0** |

### Estimated Cost Per Minute (Scaled to Paid GPU)

| GPU | Cost/hr | Processing Time/min | Cost/min of video |
|-----|---------|---------------------|-------------------|
| Colab Free (T4) | $0.00 | ~8 min | $0.00 |
| AWS g4dn.xlarge (T4) | $0.53 | ~8 min | $0.07 |
| AWS g5.xlarge (A10G) | $1.01 | ~4 min | $0.07 |
| Lambda A10 | $0.75 | ~4 min | $0.05 |

### Cost for 500 Hours of Video

| GPU Tier | Estimated Cost | Time Required |
|----------|----------------|---------------|
| A10G (single) | ~$2,100 | ~2,000 hrs |
| A10G (10x parallel) | ~$2,100 | ~200 hrs |
| A10G (50x parallel) | ~$2,100 | ~40 hrs ‚úÖ |

---

## üîß Scaling to 500 Hours Overnight

To process 500 hours of video overnight (the interview question):

### 1. Scene Detection & Segmentation
```python
# Split video at silence boundaries / shot changes
# Each segment is independently processable
from scenedetect import detect, ContentDetector
scenes = detect(video_path, ContentDetector())
```

### 2. Parallel Processing
```python
# Use multiprocessing or distributed compute
# Each scene ‚Üí separate GPU worker
from concurrent.futures import ProcessPoolExecutor

with ProcessPoolExecutor(max_workers=num_gpus) as executor:
    futures = [executor.submit(process_segment, seg) for seg in segments]
```

### 3. Infrastructure
```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Job Queue   ‚îÇ  (Redis / SQS)
                    ‚îÇ  (30K clips) ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚Üì                ‚Üì                ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ GPU Pod 1 ‚îÇ    ‚îÇ GPU Pod 2 ‚îÇ    ‚îÇ GPU Pod N ‚îÇ
    ‚îÇ (A10G)    ‚îÇ    ‚îÇ (A10G)    ‚îÇ    ‚îÇ (A10G)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì                ‚Üì                ‚Üì
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ              Object Storage (S3)              ‚îÇ
    ‚îÇ          (source + processed videos)          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4. Specific Modifications
- **Kubernetes** with GPU node pools for auto-scaling
- **Batch processing**: Group segments by duration for uniform batch sizes
- **Model server**: Keep models loaded in memory across requests (TorchServe / Triton)
- **Pipeline optimization**: Overlap stages (transcribe segment N+1 while lip-syncing segment N)
- **Checkpointing**: Resume from any failed step without re-processing

---

## üèÜ Design Decisions & Why

| Decision | Why |
|----------|-----|
| **Whisper small** (not base) | Best accuracy/VRAM trade-off for free Colab T4 |
| **IndicTrans2** (not Google) | Context-aware Hindi > literal translation. A nanny would understand it. |
| **XTTS v2** (not ElevenLabs) | Free, local, supports Hindi, voice cloning, fits on T4 |
| **Audio alignment** module | The single biggest quality improvement ‚Äî syncs lips to speech perfectly |
| **GFPGAN post-processing** | Wav2Lip blurs the face; GFPGAN restores it to near-original quality |
| **Speech rate 1.05x** | Hindi is typically 10-15% longer than English for same content |
| **Google Translate fallback** | IndicTrans2 needs ~4GB VRAM; having a zero-GPU fallback shows resourcefulness |
| **Edge TTS fallback** | Not everyone has GPU; edge-tts runs anywhere and still sounds professional |

---

## ‚ö†Ô∏è Known Limitations

1. **Wav2Lip face quality**: The GAN model improves mouth region but can still produce artifacts with fast head movements
2. **XTTS Hindi prosody**: Voice cloning works well but may not perfectly capture emotional nuances
3. **Single speaker**: Current pipeline assumes one speaker; multi-speaker support needs diarization
4. **Colab timeout**: Free Colab disconnects after ~90 minutes; long videos need batching with checkpoints
5. **Translation context**: Short clips may lose context; full-transcript translation is better

---

## üîÆ What I'd Improve With More Time

1. **VideoReTalking** instead of Wav2Lip ‚Äî better quality lip-sync, handles more poses
2. **Speaker diarization** (pyannote) for multi-speaker videos
3. **CodeFormer** instead of GFPGAN ‚Äî higher quality face restoration
4. **Segment-level TTS** ‚Äî synthesize each sentence separately for better timing alignment
5. **Emotion transfer** ‚Äî detect emotion in original speech and apply to Hindi synthesis
6. **Audio mixing** ‚Äî preserve background music/SFX and only replace speech
7. **A/B testing framework** ‚Äî automated quality metrics (SSIM, PESQ) for parameter tuning
8. **Streaming pipeline** ‚Äî process video chunks as they arrive instead of batch

---

## üìö Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| openai-whisper | ‚â•20231117 | Speech recognition |
| TTS (Coqui) | ‚â•0.22.0 | Voice cloning (XTTS v2) |
| deep-translator | ‚â•1.11.4 | Google Translate fallback |
| IndicTransToolkit | latest | IndicTrans2 translation |
| gfpgan | ‚â•1.3.8 | Face restoration |
| librosa | ‚â•0.10.0 | Audio processing/alignment |
| opencv-python | ‚â•4.8.0 | Video frame processing |
| torch | ‚â•2.0.0 | ML framework |
| edge-tts | ‚â•6.1.9 | TTS fallback (no GPU) |
| FFmpeg | ‚â•5.0 | Video/audio I/O |

---

## üìù License

MIT License ‚Äî see [LICENSE](LICENSE) for details.

---

## üôè Acknowledgments

- [OpenAI Whisper](https://github.com/openai/whisper) ‚Äî Speech recognition
- [AI4Bharat IndicTrans2](https://github.com/AI4Bharat/IndicTrans2) ‚Äî Hindi translation
- [Coqui TTS](https://github.com/coqui-ai/TTS) ‚Äî Voice cloning
- [Wav2Lip](https://github.com/Rudrabha/Wav2Lip) ‚Äî Lip synchronization
- [GFPGAN](https://github.com/TencentARC/GFPGAN) ‚Äî Face restoration
